{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "corporate-carry",
      "metadata": {
        "id": "corporate-carry"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class MultiplyGate:\n",
        "    def forward(self,W, x):\n",
        "        return np.dot(W, x)\n",
        "    def backward(self, W, x, dz):\n",
        "        dW = np.asarray(np.dot(np.transpose(np.asmatrix(dz)), np.asmatrix(x)))\n",
        "        dx = np.dot(np.transpose(W), dz)\n",
        "        return dW, dx\n",
        "\n",
        "class AddGate:\n",
        "    def forward(self, x1, x2):\n",
        "        return x1 + x2\n",
        "    def backward(self, x1, x2, dz):\n",
        "        dx1 = dz * np.ones_like(x1)\n",
        "        dx2 = dz * np.ones_like(x2)\n",
        "        return dx1, dx2\n",
        "    \n",
        "class Sigmoid:\n",
        "    def forward(self, x):\n",
        "        return 1.0 / (1.0 + np.exp(-x))\n",
        "    def backward(self, x, top_diff):\n",
        "        output = self.forward(x)\n",
        "        return (1.0 - output) * output * top_diff\n",
        "\n",
        "class Tanh:\n",
        "    def forward(self, x):\n",
        "        return np.tanh(x)\n",
        "    def backward(self, x, top_diff):\n",
        "        output = self.forward(x)\n",
        "        return (1.0 - np.square(output)) * top_diff\n",
        "    \n",
        "class Softmax:\n",
        "    def predict(self, x):\n",
        "        exp_scores = np.exp(x)\n",
        "        return exp_scores / np.sum(exp_scores)\n",
        "    def loss(self, x, y):\n",
        "        probs = self.predict(x)\n",
        "        return -np.log(probs[y])\n",
        "    def diff(self, x, y):\n",
        "        probs = self.predict(x)\n",
        "        probs[y] -= 1.0\n",
        "        return probs\n",
        "\n",
        "\n",
        "mulGate = MultiplyGate()\n",
        "addGate = AddGate()\n",
        "activation = Tanh()\n",
        "\n",
        "class RNNLayer:\n",
        "    def forward(self, x, prev_s, U, W, V):\n",
        "        self.mulu = mulGate.forward(U, x)\n",
        "        self.mulw = mulGate.forward(W, prev_s)\n",
        "        self.add = addGate.forward(self.mulw, self.mulu)\n",
        "        self.s = activation.forward(self.add)\n",
        "        self.mulv = mulGate.forward(V, self.s)\n",
        "        \n",
        "    def backward(self, x, prev_s, U, W, V, diff_s, dmulv):\n",
        "        self.forward(x, prev_s, U, W, V)\n",
        "        dV, dsv = mulGate.backward(V, self.s, dmulv)\n",
        "        ds = dsv + diff_s\n",
        "        dadd = activation.backward(self.add, ds)\n",
        "        dmulw, dmulu = addGate.backward(self.mulw, self.mulu, dadd)\n",
        "        dW, dprev_s = mulGate.backward(W, prev_s, dmulw)\n",
        "        dU, dx = mulGate.backward(U, x, dmulu)\n",
        "        return (dprev_s, dU, dW, dV)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "minimal-argentina",
      "metadata": {
        "id": "minimal-argentina"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import sys\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
        "        self.word_dim = word_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.bptt_truncate = bptt_truncate\n",
        "        self.U = np.random.uniform(-np.sqrt(1. / word_dim), np.sqrt(1. / word_dim), (hidden_dim, word_dim))\n",
        "        self.W = np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (hidden_dim, hidden_dim))\n",
        "        self.V = np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (word_dim, hidden_dim))\n",
        "        \n",
        "  \n",
        "    def forward_propagation(self, x):\n",
        "        # The total number of time steps\n",
        "        T = len(x)\n",
        "        layers = []\n",
        "        prev_s = np.zeros(self.hidden_dim)\n",
        "        # For each time step...\n",
        "        for t in range(T):\n",
        "            layer = RNNLayer()\n",
        "            input = np.zeros(self.word_dim)\n",
        "            input[x[t]] = 1\n",
        "            layer.forward(input, prev_s, self.U, self.W, self.V)\n",
        "            prev_s = layer.s\n",
        "            layers.append(layer)\n",
        "        return layers\n",
        "    \n",
        "    def predict(self, x):\n",
        "        output = Softmax()\n",
        "        layers = self.forward_propagation(x)\n",
        "        return [np.argmax(output.predict(layer.mulv)) for layer in layers]\n",
        "    \n",
        "    def calculate_loss(self, x, y):\n",
        "        assert len(x) == len(y)\n",
        "        output = Softmax()\n",
        "        layers = self.forward_propagation(x)\n",
        "        loss = 0.0\n",
        "        for i, layer in enumerate(layers):\n",
        "            loss += output.loss(layer.mulv, y[i])\n",
        "        return loss / float(len(y))\n",
        "\n",
        "    def calculate_total_loss(self, X, Y):\n",
        "        loss = 0.0\n",
        "        for i in range(len(Y)):\n",
        "            loss += self.calculate_loss(X[i], Y[i])\n",
        "        return loss / float(len(Y))\n",
        "    \n",
        "    def bptt(self, x, y):\n",
        "        assert len(x) == len(y)\n",
        "        output = Softmax()\n",
        "        layers = self.forward_propagation(x)\n",
        "        dU = np.zeros(self.U.shape)\n",
        "        dV = np.zeros(self.V.shape)\n",
        "        dW = np.zeros(self.W.shape)\n",
        "\n",
        "        T = len(layers)\n",
        "        prev_s_t = np.zeros(self.hidden_dim)\n",
        "        diff_s = np.zeros(self.hidden_dim)\n",
        "        for t in range(0, T):\n",
        "            dmulv = output.diff(layers[t].mulv, y[t])\n",
        "            input = np.zeros(self.word_dim)\n",
        "            input[x[t]] = 1\n",
        "            dprev_s, dU_t, dW_t, dV_t = layers[t].backward(input, prev_s_t, self.U, self.W, self.V, diff_s, dmulv)\n",
        "            prev_s_t = layers[t].s\n",
        "            dmulv = np.zeros(self.word_dim)\n",
        "            for i in range(t-1, max(-1, t-self.bptt_truncate-1), -1):\n",
        "                input = np.zeros(self.word_dim)\n",
        "                input[x[i]] = 1\n",
        "                prev_s_i = np.zeros(self.hidden_dim) if i == 0 else layers[i-1].s\n",
        "                dprev_s, dU_i, dW_i, dV_i = layers[i].backward(input, prev_s_i, self.U, self.W, self.V, dprev_s, dmulv)\n",
        "                dU_t += dU_i\n",
        "                dW_t += dW_i\n",
        "            dV += dV_t\n",
        "            dU += dU_t\n",
        "            dW += dW_t\n",
        "        return (dU, dW, dV)\n",
        "\n",
        "    def sgd_step(self, x, y, learning_rate):\n",
        "        dU, dW, dV = self.bptt(x, y)\n",
        "        self.U -= learning_rate * dU\n",
        "        self.V -= learning_rate * dV\n",
        "        self.W -= learning_rate * dW\n",
        "\n",
        "    def train(self, X, Y, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
        "        num_examples_seen = 0\n",
        "        losses = []\n",
        "        for epoch in range(nepoch):\n",
        "            if (epoch % evaluate_loss_after == 0):\n",
        "                loss = self.calculate_total_loss(X, Y)\n",
        "                losses.append((num_examples_seen, loss))\n",
        "                time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "                print(\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
        "                # Adjust the learning rate if loss increases\n",
        "                if len(losses) > 1 and losses[-1][1] > losses[-2][1]:\n",
        "                    learning_rate = learning_rate * 0.5\n",
        "                    print(\"Setting learning rate to %f\" % learning_rate)\n",
        "                sys.stdout.flush()\n",
        "            # For each training example...\n",
        "            for i in range(len(Y)):\n",
        "                self.sgd_step(X[i], Y[i], learning_rate)\n",
        "                num_examples_seen += 1\n",
        "        return losses "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "plain-artist",
      "metadata": {
        "id": "plain-artist"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pharmaceutical-teaching",
      "metadata": {
        "id": "pharmaceutical-teaching"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import itertools\n",
        "import nltk\n",
        "\n",
        "def getSentenceData(path, vocabulary_size=8000):\n",
        "    unknown_token = \"UNKNOWN_TOKEN\"\n",
        "    sentence_start_token = \"SENTENCE_START\"\n",
        "    sentence_end_token = \"SENTENCE_END\"\n",
        "\n",
        "    print(\"Reading CSV file...\")\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        reader = csv.reader(f, skipinitialspace=True)\n",
        "        sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\n",
        "        sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
        "    print(\"Parsed %d sentences.\" % (len(sentences)))\n",
        "\n",
        "    tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
        "    tokenized_sentences = list(filter(lambda x: len(x) > 3, tokenized_sentences))\n",
        "\n",
        "    word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
        "    print(\"Found %d unique words tokens.\" % len(word_freq.items()))\n",
        "\n",
        "    vocab = word_freq.most_common(vocabulary_size-1)\n",
        "    index_to_word = [x[0] for x in vocab]\n",
        "    index_to_word.append(unknown_token)\n",
        "    word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
        "\n",
        "    print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
        "    print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n",
        "\n",
        "    for i, sent in enumerate(tokenized_sentences):\n",
        "        tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
        "\n",
        "    print(\"\\nExample sentence: '%s'\" % sentences[1])\n",
        "    print(\"\\nExample sentence after Pre-processing: '%s'\\n\" % tokenized_sentences[0])\n",
        "\n",
        "    X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
        "    y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n",
        "\n",
        "    print(\"X_train shape: \" + str(X_train.shape))\n",
        "    print(\"y_train shape: \" + str(y_train.shape))\n",
        "\n",
        "    x_example, y_example = X_train[17], y_train[17]\n",
        "    print(\"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in x_example]), x_example))\n",
        "    print(\"\\ny:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in y_example]), y_example))\n",
        "\n",
        "    return X_train, y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "billion-living",
      "metadata": {
        "id": "billion-living",
        "outputId": "5c387493-2e08-4daf-9808-802aef8e891d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading CSV file...\n",
            "Parsed 426902 sentences.\n",
            "Found 124758 unique words tokens.\n",
            "Using vocabulary size 20000.\n",
            "The least frequent word in our vocabulary is '20-25' and appeared 8 times.\n",
            "\n",
            "Example sentence: 'SENTENCE_START why do so many women become so rude and arrogant when they get just a little bit of wealth and power? SENTENCE_END'\n",
            "\n",
            "Example sentence after Pre-processing: '['SENTENCE_START', 'why', 'do', 'so', 'many', 'women', 'become', 'so', 'rude', 'and', 'arrogant', 'when', 'they', 'get', 'just', 'a', 'little', 'bit', 'of', 'wealth', 'and', 'power', '?', 'SENTENCE_END']'\n",
            "\n",
            "X_train shape: (425628,)\n",
            "y_train shape: (425628,)\n",
            "x:\n",
            "SENTENCE_START why my answers not get any upvotes on quora ?\n",
            "[0, 19, 21, 603, 49, 38, 59, 3354, 25, 113, 2]\n",
            "\n",
            "y:\n",
            "why my answers not get any upvotes on quora ? SENTENCE_END\n",
            "[19, 21, 603, 49, 38, 59, 3354, 25, 113, 2, 1]\n",
            "2022-05-17 22:32:26: Loss after num_examples_seen=0 epoch=0: 9.903324\n",
            "2022-05-17 22:35:12: Loss after num_examples_seen=100 epoch=1: 9.883817\n",
            "2022-05-17 22:37:58: Loss after num_examples_seen=200 epoch=2: 9.851853\n",
            "2022-05-17 22:40:45: Loss after num_examples_seen=300 epoch=3: 9.785168\n",
            "2022-05-17 22:43:34: Loss after num_examples_seen=400 epoch=4: 9.636334\n",
            "2022-05-17 22:46:21: Loss after num_examples_seen=500 epoch=5: 9.314554\n",
            "2022-05-17 22:49:09: Loss after num_examples_seen=600 epoch=6: 9.048058\n",
            "2022-05-17 22:51:54: Loss after num_examples_seen=700 epoch=7: 7.996130\n",
            "2022-05-17 22:54:41: Loss after num_examples_seen=800 epoch=8: 7.057552\n",
            "2022-05-17 22:57:27: Loss after num_examples_seen=900 epoch=9: 6.537133\n"
          ]
        }
      ],
      "source": [
        "word_dim = 20000\n",
        "hidden_dim = 100\n",
        "X_train, y_train = getSentenceData('test.csv', word_dim)\n",
        "\n",
        "np.random.seed(10)\n",
        "rnn = Model(word_dim, hidden_dim)\n",
        "rnn.sgd_step(X_train[10], y_train[10], 0.005)\n",
        "\n",
        "losses = rnn.train(X_train[:100], y_train[:100], learning_rate=0.005, nepoch=10, evaluate_loss_after=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "seventh-witness",
      "metadata": {
        "id": "seventh-witness"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "silver-google",
      "metadata": {
        "id": "silver-google"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "i180617_NLP_Assignment04.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}